import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def calculate_perplexity(model, tokenizer, context, word):
    # Combine context with word
    input_text = context + " " + word
    # Encode text input to tensor
    input_ids = tokenizer.encode(input_text, return_tensors='pt')

    # Get log likelihood for each token in the text
    with torch.no_grad():
        outputs = model(input_ids, labels=input_ids)
        log_likelihood = outputs[0] * -1

    # Calculate perplexity
    perplexity = torch.exp(log_likelihood / input_ids.size(1))
    return perplexity.item()

def main():
    # Load pre-trained model and tokenizer
    model_name = "gpt2"
    model = GPT2LMHeadModel.from_pretrained(model_name)
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)

    # Example context and word
    context = "Honesty is a moral virtue."
    word = "virtue"

    # Calculate perplexity
    perplexity = calculate_perplexity(model, tokenizer, context, word)
    print(f"Perplexity of '{word}' given the context '{context}': {perplexity}")

if __name__ == "__main__":
    main()




